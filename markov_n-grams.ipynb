{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Chains - n-grams\n",
    "\n",
    "This notebook is based on the [Markov Basic Notebook](https://github.com/experimental-informatics/hands-on-text-generators/blob/master/markov_basic.ipynb).\n",
    "\n",
    "So far we have generated text with a simple vocabulary: it maps a key of one token to a value of one token.\n",
    "\n",
    "Another (maybe better method) is to use n-grams as keys and map them to a value of one token.<br>\n",
    "Then the next token prediction is based on multiple (n) tokens.\n",
    "\n",
    "![ngrams.png](images/ngrams.png)\n",
    "[Source](https://mb-14.github.io/tech/2018/10/24/gomarkov.html)\n",
    "\n",
    "Typical n-grams are of length 2 (bigrams) or 3 (trigrams).<br>\n",
    "For a small dataset trigrams may be too long, because they produce less choices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Libraries. '''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize as tok\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Set n-grams. \n",
    "Here we define the variable N_GRAMS. \n",
    "We will use it at different locatinos in our code and it must be always the same. '''\n",
    "N_GRAMS = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 50415\n",
      "['Aesthetics', ',', 'or', 'esthetics', '(', ')', ',', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste', ',', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', '(', 'its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics', ')', '.', 'It', 'examines', 'subjective', 'and', 'sensori-emotional', 'values', ',', 'or']\n"
     ]
    }
   ],
   "source": [
    "''' Read text and tokenize it with NLTK. '''\n",
    "\n",
    "# Read text\n",
    "with open('data/wiki_selection.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "# Tokenize\n",
    "token = tok(text)\n",
    "print('Number of tokens:',len(token))\n",
    "print(token[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Create pairs of tokens: n token as input (`key`) and one token as output (`value`).\n",
    "\n",
    "With the tokenization we have split our text in single tokens.<br>\n",
    "Now we have to put n tokens (as key) together.<br>\n",
    "An easy way is the usage of `' '.join(\"multiple tokens\")`.<br>\n",
    "But with this we will run into trouble with punctuation. For example the first key would be<br>\n",
    "\"Aesthetics ,\"<br>\n",
    "but it should be<br>\n",
    "\"Aesthetics,\".\n",
    "\n",
    "We will use a function found on [stackoverflow](https://stackoverflow.com/a/15950837):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some, tokens\n"
     ]
    }
   ],
   "source": [
    "''' Join tokens with space in between, except it is a punctuation. \n",
    "(We don't need to know how it works. It's enough if we see that it works.) '''\n",
    "\n",
    "def join_punctuation(seq, characters='.,;?!'):\n",
    "    characters = set(characters)\n",
    "    seq = iter(seq)\n",
    "    current = next(seq)\n",
    "\n",
    "    for nxt in seq:\n",
    "        if nxt in characters:\n",
    "            current += nxt\n",
    "        else:\n",
    "            yield current\n",
    "            current = nxt\n",
    "\n",
    "    yield current\n",
    "    \n",
    "# Usage:\n",
    "l = ['some', ',', 'tokens']\n",
    "print(' '.join(join_punctuation(l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create vocabulary. '''\n",
    "\n",
    "def make_n_gram_pairs(token, n_grams=2):\n",
    "    for i in range(len(token)-n_grams-1):\n",
    "        key = ' '.join(join_punctuation(token[i:i+n_grams]))\n",
    "        value = token[i+n_grams]\n",
    "        yield (key, value)\n",
    "\n",
    "pairs = make_n_gram_pairs(token, n_grams=N_GRAMS) # pairs is a generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aesthetics,', 'or')\n",
      "(', or', 'esthetics')\n",
      "('or esthetics', '(')\n",
      "('esthetics (', ')')\n",
      "('( )', ',')\n"
     ]
    }
   ],
   "source": [
    "''' Test pairs (execute the code above again after executing this one, otherwise the already printed pairs are gone.) '''\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(next(iter(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 29814\n"
     ]
    }
   ],
   "source": [
    "''' Create a vocabulary of all tokens and map them to their preceding tokens. '''\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "for current_token, next_token in pairs:\n",
    "    if current_token in vocabulary.keys():\n",
    "        vocabulary[current_token].append(next_token)\n",
    "    else:\n",
    "        vocabulary[current_token] = [next_token]\n",
    "        \n",
    "print('Size of the vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that our vocabulary is much longer than in the basic markov version. This is acutally not so good, because it means that our keys are more specific and less general. In reverse we will have less options for each key in our vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All options for\n",
      " Aesthetics, : ['or', 'a', '2004']\n"
     ]
    }
   ],
   "source": [
    "''' Inspect all options for a given token. '''\n",
    "key = 'Aesthetics,'\n",
    "print('All options for\\n', key, ':', vocabulary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_n_grams(input_='', n_token=12, n_grams=1):\n",
    "  \n",
    "    # get random key if input is empty\n",
    "    if input_ == '':\n",
    "        gentext = [random.choice(list(vocabulary.keys()))]\n",
    "        \n",
    "    else:\n",
    "        # tokenize input\n",
    "        gentext = tok(input_)\n",
    "\n",
    "    # predict n_token new tokens\n",
    "    for i in range(n_token):\n",
    "        \n",
    "        # token_inp = last token of gentext\n",
    "        token_inp = ' '.join(join_punctuation(gentext[-n_grams:]))\n",
    "        \n",
    "        # check if token_inp is included into the dictionary\n",
    "        if not token_inp in vocabulary.keys():\n",
    "            # pick a random choice if not included\n",
    "            token_inp = random.choice(list(vocabulary.keys()))\n",
    "            \n",
    "        # get all options for the last token of gentext\n",
    "        options = vocabulary[token_inp]\n",
    "        # choose one of this options\n",
    "        choice = np.random.choice(options)\n",
    "        # append it to the generated text\n",
    "        gentext.append(choice)\n",
    "        \n",
    "    \n",
    "    # when the for-loop is finised, creat the output\n",
    "    output = ''\n",
    "    for token in gentext:\n",
    "        if token in string.punctuation:\n",
    "            output += token\n",
    "        else:\n",
    "            # add a whitespace if token is not a punctuation\n",
    "            output += ' ' + token\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " them learn the European context, a number of specific instances of a synthetic/abstract\n",
      " artificial minds. symptoms examples. Loss functions express the discrepancy between the philosophy\n",
      " a service. supervised is but of any moral or political purpose. Hence\n"
     ]
    }
   ],
   "source": [
    "''' The function above allows the text genration without text input. '''\n",
    "\n",
    "for i in range(3):\n",
    "    print(generate_text_n_grams(n_grams=N_GRAMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aesthetics, or suchâ€”not worthy of consideration. Learning classifier systems( LCS)\n",
      " Aesthetics, 2004. Thus, judgments of truth both are to be adopted\n",
      " Aesthetics, or likely true. Conversely, machine learning algorithms build a mathematical\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(generate_text_n_grams('Aesthetics,', 12, N_GRAMS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Further tasks/ experiments\n",
    "\n",
    "- Try it with n_grams = 3\n",
    "- Try it without punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "\n",
    "https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "https://mb-14.github.io/tech/2018/10/24/gomarkov.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
