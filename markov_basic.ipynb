{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Chains\n",
    "\n",
    "![markov_chain.jpg](images/markov_chain.jpg)\n",
    "[Source](https://mb-14.github.io/tech/2018/10/24/gomarkov.html)\n",
    "\n",
    "<br>\n",
    "\n",
    "Markov chains are a simple way of **next value prediction**. This value may be for e.g. a next word, the next number or some next decision.\n",
    "\n",
    "All (historic) information used to compute this next value lies in the current value. So the next value is not based on a sequence of previous values, but just on the one current value.\n",
    "\n",
    "(In this implementation this algorithm can't learn anything!)\n",
    "\n",
    "Procedure: \n",
    "- create a dictionary of all words in a given text\n",
    "- store for each word all the direct following words\n",
    "- if a word appears as input: lookup the word in the dictionary and choose one of the options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Libraries. '''\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize as tok\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "The easiest way to tokenize a text is with the built-in function `split()`. But punctuation like .!, isn't separated from its preceding word. A more sophisticated solution is the usage of a library like nltk or spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 44032\n",
      "['Aesthetics,', 'or', 'esthetics', '(),', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste,', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', '(its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics).', 'It', 'examines', 'subjective', 'and', 'sensori-emotional', 'values,', 'or', 'sometimes', 'called', 'judgments', 'of', 'sentiment', 'and', 'taste.Aesthetics', 'covers']\n"
     ]
    }
   ],
   "source": [
    "''' Read text and tokenize it with split(). '''\n",
    "# Read text\n",
    "with open('data/wiki_selection.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "# Split into single words (tokens)\n",
    "token = text.split()\n",
    "print('Number of tokens:',len(token))\n",
    "print(token[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 50415\n",
      "['Aesthetics', ',', 'or', 'esthetics', '(', ')', ',', 'is', 'a', 'branch', 'of', 'philosophy', 'that', 'deals', 'with', 'the', 'nature', 'of', 'beauty', 'and', 'taste', ',', 'as', 'well', 'as', 'the', 'philosophy', 'of', 'art', '(', 'its', 'own', 'area', 'of', 'philosophy', 'that', 'comes', 'out', 'of', 'aesthetics', ')', '.', 'It', 'examines', 'subjective', 'and', 'sensori-emotional', 'values', ',', 'or']\n"
     ]
    }
   ],
   "source": [
    "''' Read text and tokenize it with NLTK. '''\n",
    "token = tok(text)\n",
    "print('Number of tokens:',len(token))\n",
    "print(token[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabulary\n",
    "\n",
    "Create pairs of tokens: one token as input and the preceding token as a possible output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a generator with pairs of tokens. '''\n",
    "\n",
    "def make_pairs(token):\n",
    "    for i in range(len(token)- 1):\n",
    "        yield (token[i], token[i+1:i+2])\n",
    "\n",
    "pairs = make_pairs(token) # pairs is a generator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Aesthetics', [','])\n",
      "(',', ['or'])\n",
      "('or', ['esthetics'])\n"
     ]
    }
   ],
   "source": [
    "''' Test pairs (execute the code above again after executing this one, otherwise the already printed pairs are gone.) '''\n",
    "\n",
    "# for i in range(3):\n",
    "#     print(next(iter(pairs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Create a vocabulary of all tokens and map them to their preceding tokens. '''\n",
    "\n",
    "# Create an empty dictionary.\n",
    "vocabulary = {}\n",
    "\n",
    "# Iterate through the pairs created above.\n",
    "for current_token, next_token in pairs:\n",
    "    # Check if the current token is already included into the dictionary.\n",
    "    if current_token in vocabulary.keys():\n",
    "        # If yes, append the next token to this entry.\n",
    "        vocabulary[current_token].append(' '.join(next_token))\n",
    "    else:\n",
    "        # Otherwise create a new entry with the current token.\n",
    "        vocabulary[current_token] = [' '.join(next_token)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the vocabulary: 6946\n"
     ]
    }
   ],
   "source": [
    "print('Size of the vocabulary:', len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All options for\n",
      " Aesthetics : [',', 'in', ',', 'and', 'is', 'encompasses', 'examines', 'is', ',', '.', 'and']\n"
     ]
    }
   ],
   "source": [
    "''' Inspect all options for a given token. '''\n",
    "key = token[0]\n",
    "print('All options for\\n', key, ':', vocabulary[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you may see some tokens appear multiple times. This is an easy way to assure that the probabilty for that token is higher."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate text\n",
    "\n",
    "The following function takes a text sequence (or just one token) as input.<br>\n",
    "It starts with the last token of the input and looks in the dictionary for all possible next tokens.<br>\n",
    "Then one of this options is picked with the function `np.random.choice()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Function to generate n next token. '''\n",
    "\n",
    "def generate_text(input_, n_token=1):\n",
    "    # input_ = string of text. Arbitrary length, but only the last token is used for the prediction.\n",
    "    # n_token = number of tokens that the function generates.\n",
    "    \n",
    "    # tokenize input\n",
    "    gentext = tok(input_)\n",
    "    \n",
    "    # predict n_token new tokens\n",
    "    for i in range(n_token):\n",
    "        # token = last token of gentext\n",
    "        token_inp = gentext[-1]\n",
    "        # check if token is included into the dictionary\n",
    "        if not token_inp in vocabulary.keys():\n",
    "            # pick a random choice if not included\n",
    "            token_inp = random.choice(list(vocabulary.keys()))\n",
    "        # get all options for the last token of gentext\n",
    "        # it is assumed that this token exists in the vocabulary\n",
    "        options = vocabulary[token_inp]\n",
    "        # choose one of this options\n",
    "        choice = np.random.choice(options)\n",
    "        # append it to the generated text\n",
    "        gentext.append(choice)\n",
    "    \n",
    "    # create output\n",
    "    output = ''\n",
    "    # loop through all predicted tokens\n",
    "    for token in gentext:\n",
    "        # if token is a punctuation:\n",
    "        if token in string.punctuation:\n",
    "            # append it without a whitespace\n",
    "            output += token\n",
    "        else:\n",
    "            # else add a whitespace before the token\n",
    "            output += ' ' + token\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The artistic code, because there are the idea that they were a status of\n"
     ]
    }
   ],
   "source": [
    "''' Generated text. '''\n",
    "\n",
    "# Pick a random input\n",
    "key = token[random.randint(0, len(token))]\n",
    "# Or use a string as input\n",
    "key = 'The artistic code' \n",
    "\n",
    "generated_text = generate_text(key, 12)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The artistic code to the relations. An Introduction Defining science, W.V. If the present and\n",
      " The artistic code to train search for Drawing Caricaturas: Clarendon. Criticism, artificial neurons '' model\n",
      " The artistic code, such as well as a field of questions. Reinforcement learning: a sparse\n",
      " The artistic code to understand how facts and understand how they help us to enlarge their human intelligence\n",
      " The artistic code, as Abu Hamid Muhammad al-Ghazali that were among several distinctive explanations are countless subjects\n"
     ]
    }
   ],
   "source": [
    "''' Generate multiple texts at once to see the differences. '''\n",
    "for i in range(5):\n",
    "    print(generate_text(key, 15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources\n",
    "\n",
    "https://towardsdatascience.com/simulating-text-with-markov-chains-in-python-1a27e6d13fc6\n",
    "\n",
    "https://mb-14.github.io/tech/2018/10/24/gomarkov.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
